

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Custom Training With PixelLib &mdash; PixelLib 0.4.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inference With A Custom Model" href="custom_inference.html" />
    <link rel="prev" title="Instance segmentation of videos with PixelLib" href="video_instance.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> PixelLib
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="image_ade20k.html"><strong>Semantic segmentation of images with PixelLib using Ade20k model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_ade20k.html"><strong>Semantic segmentation of videos with PixelLib using Ade20k model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_ade20k.html#segmentation-of-live-camera-with-ade20k-model"><strong>Segmentation of live camera with Ade20k model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_pascal.html"><strong>Semantic segmentation of images with PixelLib using Pascalvoc model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_pascal.html"><strong>Semantic Segmentation of videos with PixelLib using Pascalvoc model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_pascal.html#segmentation-of-live-camera-with-pascalvoc-model"><strong>Segmentation of live camera with pascalvoc model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Image_instance.html"><strong>Instance segmentation of images with PixelLib</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_instance.html"><strong>Instance segmentation of videos with PixelLib</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="video_instance.html#instance-segmentation-of-live-camera-with-mask-r-cnn"><strong>Instance Segmentation of Live Camera with Mask R-cnn.</strong></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><strong>Custom Training With PixelLib</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_inference.html"><strong>Inference With A Custom Model</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="change_image_bg.html">Image Tuning With PixelLib</a></li>
<li class="toctree-l1"><a class="reference internal" href="change_video_bg.html">Change the Background of A Video</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PixelLib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li><strong>Custom Training With PixelLib</strong></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/custom_train.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="custom-training-with-pixellib">
<span id="custom-train"></span><h1><strong>Custom Training With PixelLib</strong><a class="headerlink" href="#custom-training-with-pixellib" title="Permalink to this headline">¶</a></h1>
<p>Implement custom training on your own dataset using PixelLib’s Library. In just seven Lines of code you can create a custom model for perform instance segmentation and object detection for your own application.</p>
<p><strong>Prepare your dataset</strong></p>
<p>Our goal is to create a model that can perform instance segmentation and object detection on butterflies and squirrels.
Collect images for the objects you want to detect and annotate your dataset for custom training.
Labelme is the tool employed to perform polygon annotation of objects.
Create a root directory or folder and within it create train and test folder.
Separate the images required for training (a minimum of 300) and test.
Put the images you want to use for training in the train folder and put the images you want to use for testing in the test folder.
You will annotate both images in the train and test folder. Download <a class="reference external" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.0.0/Nature.zip">Nature’s dataset</a> used as a sample dataset in this article,
unzip it to extract the images’ folder. This dataset will serve as a guide for you to know how to organize your images.
Ensure that the format of the directory of your own dataset directory is not different from it. Nature is a dataset with two categories butterfly and squirrel. There is 300 images for each class for training and 100 images for each class for testing i.e 600 images for training and 200 images for validation. Nature is a dataset with 800 images.</p>
<p>Read this article on <a class="reference external" href="https://medium.com/&#64;olafenwaayoola/image-annotation-with-labelme-81687ac2d077">medium</a> and learn how to annotate objects with <em>Labelme</em>.</p>
<p><strong>Note:</strong>
Use labelme for annotation of objects. If you use a different annotation tool it will not be compatible with the library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Nature</span> <span class="o">&gt;&gt;</span><span class="n">train</span><span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span> <span class="n">image1</span><span class="o">.</span><span class="n">jpg</span>
                           <span class="n">image1</span><span class="o">.</span><span class="n">json</span>
                           <span class="n">image2</span><span class="o">.</span><span class="n">jpg</span>
                           <span class="n">image2</span><span class="o">.</span><span class="n">json</span>

    <span class="o">&gt;&gt;</span><span class="n">test</span><span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span> <span class="n">img1</span><span class="o">.</span><span class="n">jpg</span>
                           <span class="n">img1</span><span class="o">.</span><span class="n">json</span>
                           <span class="n">img2</span><span class="o">.</span><span class="n">jpg</span>
                           <span class="n">img2</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Sample of folder directory after annotation.</p>
<p><strong>Visualize Dataset</strong></p>
<p>Visualize a sample image before training to confirm that the masks and bounding boxes are well generated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>

<span class="n">vis_img</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">vis_img</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">vis_img</span><span class="o">.</span><span class="n">visualize_sample</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>
<span class="n">vis_img</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
</pre></div>
</div>
<p>We imported in pixellib, from pixellib import the class instance_custom_training and created an instance of the class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vis_img</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We loaded the dataset using <em>load_dataset function</em>. PixelLib requires polygon annotations to be in coco format, when you call the <em>load_data function</em> the individual json files in the train and test folder will be converted into a single <em>train.json</em> and <em>test.json</em> respectively. The train and test json files will be located in the root directory as the train and test folder. The new folder directory will now look like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Nature</span> <span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><span class="n">train</span><span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span> <span class="n">image1</span><span class="o">.</span><span class="n">jpg</span>
           <span class="n">train</span><span class="o">.</span><span class="n">json</span>               <span class="n">image1</span><span class="o">.</span><span class="n">json</span>
                                    <span class="n">image2</span><span class="o">.</span><span class="n">jpg</span>
                                    <span class="n">image2</span><span class="o">.</span><span class="n">json</span>

   <span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><span class="n">test</span><span class="o">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span> <span class="n">img1</span><span class="o">.</span><span class="n">jpg</span>
              <span class="n">test</span><span class="o">.</span><span class="n">json</span>             <span class="n">img1</span><span class="o">.</span><span class="n">json</span>
                                    <span class="n">img2</span><span class="o">.</span><span class="n">jpg</span>
                                    <span class="n">img2</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Inside the load_dataset function annotations are extracted from the jsons’s files. Bitmap masks are generated from the polygon points of the annotations and bounding boxes are generated from the masks. The smallest box that encapsulates all the pixels of the mask is used as a bounding box.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vis_img</span><span class="o">.</span><span class="n">visualize_sample</span><span class="p">()</span>
</pre></div>
</div>
<p>When you called this function it shows a sample image with a mask and bounding box.</p>
<img alt="_images/sq_sample.png" src="_images/sq_sample.png" />
<img alt="_images/but_sample.png" src="_images/but_sample.png" />
<p>Great! the dataset is fit for training, the load_dataset function successfully generates mask and bounding box for each object in the image. Random colors are generated for the masks in HSV space and then converted to RGB.</p>
<p><strong>Train a custom model Using your dataset</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>

<span class="n">train_maskrcnn</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet101&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_pretrained_model</span><span class="p">(</span><span class="s2">&quot;mask_rcnn_coco.h5&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="n">path_trained_models</span> <span class="o">=</span> <span class="s2">&quot;mask_rcnn_models&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This is the code for performing training, in just seven lines of code you train your dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet101&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>We called the function modelConfig, i.e model’s configuration. It takes the following parameters:</p>
<p><strong>network_backbone:</strong> This the CNN network used as a feature extractor for mask-rcnn. The feature extractor used is resnet101.</p>
<p><strong>num_classes:</strong>  We set the number of classes to the categories of objects in the dataset. In this case we have two classes(butterfly and squirrel) in nature’s dataset.</p>
<p><strong>batch_size:</strong> This is the batch size for training the model. It is set to 4.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_pretrained_model</span><span class="p">(</span><span class="s2">&quot;mask_rcnn_coco.h5&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We are going to employ the technique of transfer learning for training the model. Coco model has been trained on 8O categories of objects, it has learnt a lot of features that will help in training the model. We called the function load_pretrained_model function to load the mask-rcnn coco model.We loaded the dataset using <em>load_dataset function</em>.</p>
<p>Download coco model from <a class="reference external" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.2/mask_rcnn_coco.h5)">here</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">path_trained_models</span> <span class="o">=</span> <span class="s2">&quot;mask_rcnn_models&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we called the train function for training maskrcnn model. We called <em>train_model function</em>.  The function takes the following parameters:</p>
<p><strong>num_epochs:</strong> The number of epochs required for training the model. It is set to 300.</p>
<p><strong>augmentation:</strong> Data augmentation is applied on the dataset, this is because we want the model to learn different representations of the objects.</p>
<p><strong>path_trained_models:</strong> This is the path to save the trained models during training. Models with the lowest validation losses are saved.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">resnet101</span> <span class="k">as</span> <span class="n">network</span> <span class="n">backbone</span> <span class="n">For</span> <span class="n">Mask</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="n">model</span>
<span class="n">Train</span> <span class="mi">600</span> <span class="n">images</span>
<span class="n">Validate</span> <span class="mi">200</span> <span class="n">images</span>
<span class="n">Applying</span> <span class="n">augmentation</span> <span class="n">on</span> <span class="n">dataset</span>
<span class="n">Checkpoint</span> <span class="n">Path</span><span class="p">:</span> <span class="n">mask_rcnn_models</span>
<span class="n">Selecting</span> <span class="n">layers</span> <span class="n">to</span> <span class="n">train</span>
<span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">100</span><span class="o">/</span><span class="mi">100</span> <span class="o">-</span> <span class="mi">164</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">2.2184</span> <span class="o">-</span> <span class="n">rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0174</span> <span class="o">-</span> <span class="n">rpn_bbox_loss</span><span class="p">:</span> <span class="mf">0.8019</span> <span class="o">-</span> <span class="n">mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1655</span> <span class="o">-</span> <span class="n">mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.7274</span> <span class="o">-</span> <span class="n">mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.5062</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">2.5806</span> <span class="o">-</span> <span class="n">val_rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0221</span> <span class="o">-</span> <span class="n">val_rpn_bbox_loss</span><span class="p">:</span> <span class="mf">1.4358</span> <span class="o">-</span> <span class="n">val_mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1574</span> <span class="o">-</span> <span class="n">val_mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.6080</span> <span class="o">-</span> <span class="n">val_mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.3572</span>

<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">100</span><span class="o">/</span><span class="mi">100</span> <span class="o">-</span> <span class="mi">150</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.4641</span> <span class="o">-</span> <span class="n">rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0126</span> <span class="o">-</span> <span class="n">rpn_bbox_loss</span><span class="p">:</span> <span class="mf">0.5438</span> <span class="o">-</span> <span class="n">mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1510</span> <span class="o">-</span> <span class="n">mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.4177</span> <span class="o">-</span> <span class="n">mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.3390</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">1.2217</span> <span class="o">-</span> <span class="n">val_rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0115</span> <span class="o">-</span> <span class="n">val_rpn_bbox_loss</span><span class="p">:</span> <span class="mf">0.4896</span> <span class="o">-</span> <span class="n">val_mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1542</span> <span class="o">-</span> <span class="n">val_mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.3111</span> <span class="o">-</span> <span class="n">val_mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.2554</span>

<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">200</span>
<span class="mi">100</span><span class="o">/</span><span class="mi">100</span> <span class="o">-</span> <span class="mi">145</span><span class="n">s</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1.0980</span> <span class="o">-</span> <span class="n">rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0118</span> <span class="o">-</span> <span class="n">rpn_bbox_loss</span><span class="p">:</span> <span class="mf">0.4122</span> <span class="o">-</span> <span class="n">mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1271</span> <span class="o">-</span> <span class="n">mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.2860</span> <span class="o">-</span> <span class="n">mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.2609</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">1.0708</span> <span class="o">-</span> <span class="n">val_rpn_class_loss</span><span class="p">:</span> <span class="mf">0.0149</span> <span class="o">-</span> <span class="n">val_rpn_bbox_loss</span><span class="p">:</span> <span class="mf">0.3645</span> <span class="o">-</span> <span class="n">val_mrcnn_class_loss</span><span class="p">:</span> <span class="mf">0.1360</span> <span class="o">-</span> <span class="n">val_mrcnn_bbox_loss</span><span class="p">:</span> <span class="mf">0.3059</span> <span class="o">-</span> <span class="n">val_mrcnn_mask_loss</span><span class="p">:</span> <span class="mf">0.2493</span>
</pre></div>
</div>
<p>This is the training log it shows the network backbone used for training mask-rcnn which is <em>resnet101</em>, the number of images used for training and number of images used for validation. In the <em>path_to_trained models’s</em> directory the models are saved based on decrease in validation loss, typical model name will appear like this: <strong>mask_rcnn_model_25–0.55678</strong>, it is saved with its <em>epoch number</em> and its corresponding <em>validation loss</em>.</p>
<p>Network Backbones:
There are two network backbones for training mask-rcnn</p>
<p><strong>1. Resnet101</strong></p>
<p><strong>2. Resnet50</strong></p>
<p><strong>Google colab:</strong> Google Colab provides a single 12GB NVIDIA Tesla K80 GPU that can be used up to 12 hours continuously.</p>
<p><strong>Using Resnet101:</strong> Training Mask-RCNN consumes alot of memory. On google colab using resnet101 as network backbone you will be able to train with a batchsize of 4. The default network backbone is resnet101. Resnet101 is used as a default backbone because it appears to reach a lower validation loss during training faster than resnet50. It also works better for a dataset with multiple classes and much more images.</p>
<p><strong>Using Resnet50:</strong> The advantage with resnet50 is that it consumes lesser memory, you can use a batch_size of 6 0r 8 on google colab depending on how colab randomly allocates gpu.
The modified code supporting resnet50 will be like this.</p>
<p>Full code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>

<span class="n">train_maskrcnn</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_pretrained_model</span><span class="p">(</span><span class="s2">&quot;mask_rcnn_coco.h5&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">train_model</span><span class="p">(</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">augmentation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">path_trained_models</span> <span class="o">=</span> <span class="s2">&quot;mask_rcnn_models&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The main differences from the original code is that in the model configuration function we set network_backbone to be <em>resnet50</em> and changed the batch size to 6.</p>
<p>The only difference in the training log is this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Using</span> <span class="n">resnet50</span> <span class="k">as</span> <span class="n">network</span> <span class="n">backbone</span> <span class="n">For</span> <span class="n">Mask</span> <span class="n">R</span><span class="o">-</span><span class="n">CNN</span> <span class="n">model</span>
</pre></div>
</div>
<p>It shows that we are using <em>resnet50</em> for training.</p>
<p><strong>Note:</strong> The batch_sizes given are samples used for google colab. If you are using a less powerful GPU, reduce your batch size, for example a PC with a 4G RAM GPU you should use a batch size of 1 for both resnet50 or resnet101. I used a batch size of 1 to train my model on my PC’s GPU, train for less than 100 epochs and it produced a validation loss of 0.263. This is favourable because my dataset is not large. A PC with a more powerful GPU you can use a batch size of 2. If you have a large dataset with more classes and much more images use google colab where you have free access to a single 12GB NVIDIA Tesla K80 GPU that can be used up to 12 hours continuously. Most importantly try and use a more powerful GPU and train for more epochs to produce a custom model that will perform efficiently across multiple classes. Achieve better results by training with much more images. 300 images for each each class is recommended to be the minimum required for training.</p>
<p><strong>Model Evaluation</strong></p>
<p>When we are done with training we should evaluate models with lowest validation losses.
Model evaluation is used to access the performance of the trained model on the test dataset.
Download the trained model from <a class="reference external" href="https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.0.0/Nature_model_resnet101.h5">here</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>


<span class="n">train_maskrcnn</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet101&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="s2">&quot;mask_rccn_models/Nature_model_resnet101.h5&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>output</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask_rcnn_models</span><span class="o">/</span><span class="n">Nature_model_resnet101</span><span class="o">.</span><span class="n">h5</span> <span class="n">evaluation</span> <span class="n">using</span> <span class="n">iou_threshold</span> <span class="mf">0.5</span> <span class="ow">is</span> <span class="mf">0.890000</span>
</pre></div>
</div>
<p>The mAP(Mean Avearge Precision) of the model is <em>0.89</em>.</p>
<p>You can evaluate multiple models at once, what you just need is to pass in the folder directory of the models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>


<span class="n">train_maskrcnn</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet101&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="s2">&quot;mask_rccn_models&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Output log</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask_rcnn_models</span>\<span class="n">Nature_model_resnet101</span><span class="o">.</span><span class="n">h5</span> <span class="n">evaluation</span> <span class="n">using</span> <span class="n">iou_threshold</span> <span class="mf">0.5</span> <span class="ow">is</span> <span class="mf">0.890000</span>

<span class="n">mask_rcnn_models</span>\<span class="n">mask_rcnn_model_055</span><span class="o">.</span><span class="n">h5</span> <span class="n">evaluation</span> <span class="n">using</span> <span class="n">iou_threshold</span> <span class="mf">0.5</span> <span class="ow">is</span> <span class="mf">0.867500</span>

<span class="n">mask_rcnn_models</span>\<span class="n">mask_rcnn_model_058</span><span class="o">.</span><span class="n">h5</span> <span class="n">evaluation</span> <span class="n">using</span> <span class="n">iou_threshold</span> <span class="mf">0.5</span> <span class="ow">is</span> <span class="mf">0.8507500</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pixellib</span>
<span class="kn">from</span> <span class="nn">pixellib.custom_train</span> <span class="kn">import</span> <span class="n">instance_custom_training</span>


<span class="n">train_maskrcnn</span> <span class="o">=</span> <span class="n">instance_custom_training</span><span class="p">()</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">modelConfig</span><span class="p">(</span><span class="n">network_backbone</span> <span class="o">=</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Nature&quot;</span><span class="p">)</span>
<span class="n">train_maskrcnn</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="s2">&quot;path_to_model path or models&#39;s folder directory&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note:</strong> Change the network_backbone to resnet50 if you are evaluating a resnet50 model.</p>
<p>Visit <a class="reference external" href="https://colab.research.google.com/drive/1LIhBcxF6TUQUQCMEXCRBuF4a7ycUmjuw?usp=sharing">Google Colab’s notebook</a> set up for training a custom dataset</p>
<p>Learn how how to perform inference with your custom model by reading this <a class="reference external" href="https://pixellib.readthedocs.io/en/latest/custom_inference.html">tutorial</a></p>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="custom_inference.html" class="btn btn-neutral float-right" title="Inference With A Custom Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="video_instance.html" class="btn btn-neutral float-left" title="Instance segmentation of videos with PixelLib" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ayoola Olafenwa

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>